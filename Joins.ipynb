{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tClHUD-sGrAY",
        "outputId": "f07ad795-f6df-4473-e000-06a8299f4c8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 48 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 52.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=586603f448b742f400f83409148154b18d530dcaabdd4043457f129906fd3b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-d85mNGMgf"
      },
      "source": [
        "### Joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5TQleB0GMgp"
      },
      "source": [
        "In a Spark application, you use the PySpark JOINS operation to join multiple dataframes.\n",
        "\n",
        " \n",
        "\n",
        "The concept of a join operation is to join and merge or extract data from two different dataframes or data sources. You use the join operation in Spark to join rows in a dataframe based on relational columns. It adds the data that satisfies the relation to the range but excludes the data that does not satisfy the relation.\n",
        "There are various types of PySpark JOINS that allow you to join numerous datasets and manipulate them as needed. The following are the most commonly used join operations:-\n",
        "\n",
        " \n",
        "\n",
        "* Inner Join,\n",
        "\n",
        "* Full Outer Join,\n",
        "\n",
        "* Right Outer Join,\n",
        "\n",
        "* Left Outer Join,\n",
        "\n",
        "* Left Semi Join, etc.\n",
        "\n",
        "### General Syntax for PySpark Join-\n",
        " \n",
        "\n",
        "##### join(self, other, on=None, how=None). \n",
        "\n",
        " \n",
        "\n",
        "The PySpark join operation takes the following parameters. It returns a single DataFrame as a result-\n",
        "\n",
        " \n",
        "\n",
        "* other- Dataframe on right side of the join operation\n",
        "\n",
        "* on- a string for the joining column name\n",
        "\n",
        "* how-  Inner, outer, full, full outer, left, left outer, right, right outer, left semi, and left anti are the only options. By default, its value is ‘inner’.\n",
        "\n",
        "Let us now look at the various PySpark Join types and their syntax and examples.\n",
        "\n",
        "Before diving into the PySpark Join types, we first create two datasets/tables- Emp and Dept. Column \"emp_id\" is unique in the emp dataset, and \"dept id\" is unique in the dept dataset. Also, the emp dataset's emp_dept_id has a relation to the dept dataset's dept_id.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEDh-OOJGMgt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7K8010HwGMgw"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Joins\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q3uVoomyGMgy"
      },
      "outputs": [],
      "source": [
        "emp = [(1,\"John\",\"2018\",\"10\",\"M\",3000), \\\n",
        "\n",
        "    (2,\"Dario\",\"2010\",\"20\",\"M\",4000), \\\n",
        "\n",
        "    (3,\"Ross\",\"2010\",\"10\",\"M\",1000), \\\n",
        "\n",
        "    (4,\"Rachel\",\"2005\",\"40\",\"F\",2000), \\\n",
        "\n",
        "    (5,\"Monica\",\"2010\",\"50\",\"F\",3000), \\\n",
        "\n",
        " ]\n",
        "empColumns = [\"emp_id\",\"name\",\"year_joined\", \\\n",
        "\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGGvTWIsGMg1",
        "outputId": "24c83b29-6028-4e08-a38d-af6044720dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+------+-----------+-----------+------+------+\n",
            "|emp_id|name  |year_joined|emp_dept_id|gender|salary|\n",
            "+------+------+-----------+-----------+------+------+\n",
            "|1     |John  |2018       |10         |M     |3000  |\n",
            "|2     |Dario |2010       |20         |M     |4000  |\n",
            "|3     |Ross  |2010       |10         |M     |1000  |\n",
            "|4     |Rachel|2005       |40         |F     |2000  |\n",
            "|5     |Monica|2010       |50         |F     |3000  |\n",
            "+------+------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "\n",
        "empDF.printSchema()\n",
        "\n",
        "empDF.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Rs9_DRW3GMg5"
      },
      "outputs": [],
      "source": [
        "dept = [(\"Finance\",10), \\\n",
        "\n",
        "    (\"Marketing\",20), \\\n",
        "\n",
        "    (\"Sales\",30), \\\n",
        "\n",
        "    (\"IT\",40) \\\n",
        "\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WidN1NwyGMg6",
        "outputId": "7630902e-90a1-46d7-af62-5e353e2f7639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "\n",
        "deptDF.printSchema()\n",
        "\n",
        "deptDF.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sewF2GMSGMg8"
      },
      "source": [
        "### PySpark Inner Join\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "This is Apache Spark's default join type. The inner join removes everything that isn't common in both tables. It returns all data from both sides of the table that matches the join condition (predicate in the 'on' parameter). As a result, if one of the tables is empty, the result will be empty too.\n",
        "\n",
        " \n",
        "\n",
        "The syntax for PySpark Inner join is as follows -\n",
        "\n",
        " \n",
        "\n",
        "table1.join(table2,table1.column_name == table2.column_name,”inner”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAla-infGMg9",
        "outputId": "d5b7ae06-df00-46ba-8075-48907eb02ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqhi37gTGMg_"
      },
      "source": [
        "### PySpark Left Join / PySpark Left Outer Join\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The PySpark outer join enables you to include rows from one table in the result set even if it cannot identify any matching rows from another table.\n",
        "\n",
        " \n",
        "\n",
        "Even when there isn't a match in the right table, all left table rows remain unaffected in a PySpark left outer join. If there is a match in the right table, it returns the matching rows.\n",
        "\n",
        " \n",
        "\n",
        "When the join expression doesn't match, it assigns null to that record and eliminates records from the correct dataset.\n",
        "\n",
        " \n",
        "\n",
        "### The syntax for PySpark Left Outer join-\n",
        "\n",
        "* left: table1.join(table2,table1.column_name ==  table2.column_name,”left”)\n",
        "\n",
        "* leftouter: table1.join(table2,table1.column_name ==  table2.column_name,”leftouter”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEhS0Tj4GMhA",
        "outputId": "3d6987f3-6d8d-414d-ab15-25ac29769c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|     5|Monica|       2010|         50|     F|  3000|     null|   null|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz9EjH3cGMhA",
        "outputId": "1bd491b0-dfe1-49f5-da49-51a503393164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|     5|Monica|       2010|         50|     F|  3000|     null|   null|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXSu3okGMhB"
      },
      "source": [
        "### PySpark Right Join/ PySpark Right Outer Join\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "PySpark right outer join is the complete opposite of left join in that it returns all rows from the right dataset irrespective of match found on the left dataset. When join expressions don't match, it assigns null to that record and eliminates records from the left dataset. The output dataset is formed by joining all rows from the second dataset and only matching rows from the first dataset with respect to the second dataset.\n",
        "\n",
        " \n",
        "\n",
        "The syntax for PySpark Right Outer join-\n",
        "\n",
        " \n",
        "\n",
        "right: table1.join(table2,table1.column_name ==  table2.column_name,”right”)\n",
        "\n",
        "rightouter: table1.join(table2,table1.column_name == table2.column_name,”rightouter”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EApp4LdUGMhC",
        "outputId": "fd95d03e-9e2a-447a-99b2-a3108d33954d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  null|  null|       null|       null|  null|  null|    Sales|     30|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-qcToaCGMhC",
        "outputId": "aab53e55-d6c2-485c-da93-9d6eaadff0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  null|  null|       null|       null|  null|  null|    Sales|     30|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightOuter\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HzccS-TGMhD"
      },
      "source": [
        "### PySpark Full Outer Join\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "PySpark full outer join is used to keep records from both tables along with the associated zero values in the left/right tables. It is a rather unusual occurrence, but it's usually employed when you don't want to delete data from either table. If the join expression does not match, the record columns are null.\n",
        "\n",
        " \n",
        "\n",
        "You can perform this join using three different keywords- outer, full, and fullouter.\n",
        "\n",
        " \n",
        "\n",
        "The syntax for PySpark Full Outer join is as follows-\n",
        "\n",
        " \n",
        "\n",
        "outer: table1.join(table2,table1.column_name ==  table2.column_name,”outer”)\n",
        "\n",
        "full: table1.join(table2,table1.column_name == table2.column_name,”full”)\n",
        "\n",
        "fullouter: table1.join(table2,table1.column_name ==  table2.column_name,”fullouter”)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_3C6D8-GMhE",
        "outputId": "32a35d00-d703-46bb-c831-33a813b92e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  null|  null|       null|       null|  null|  null|    Sales|     30|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "|     5|Monica|       2010|         50|     F|  3000|     null|   null|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYRng8RZGMhE",
        "outputId": "cfcc5f76-dd7c-4d58-924c-b5793c7f0793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  null|  null|       null|       null|  null|  null|    Sales|     30|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "|     5|Monica|       2010|         50|     F|  3000|     null|   null|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCkqymgOGMhF",
        "outputId": "cc49decf-fa39-4e52-854e-21279b07178c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|  name|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|  John|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|  Ross|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     2| Dario|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  null|  null|       null|       null|  null|  null|    Sales|     30|\n",
            "|     4|Rachel|       2005|         40|     F|  2000|       IT|     40|\n",
            "|     5|Monica|       2010|         50|     F|  3000|     null|   null|\n",
            "+------+------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullOuter\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3uncy9lGMhG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}