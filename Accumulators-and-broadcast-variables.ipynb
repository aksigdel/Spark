{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khqMkA4uAgoj",
        "outputId": "a2b238cd-10d3-45dd-fe6e-11269400023f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 56 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 70.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=e74467ff3b06788f517b97519d821d27dd657f1c93dfcbc8590b372a7dcc99e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgpPsmDhAa0i"
      },
      "source": [
        "# Accumulators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Gy6Te1Aa0t"
      },
      "source": [
        "Accumulators are a special kind of variable that we basically use to update some data points across executors. One thing we really need to remember is that the operation by which the data point update happens has to be an associated and commutative operation. \n",
        "\n",
        "Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
        "\n",
        "As a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance counter) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsUjVXE1Aa0y"
      },
      "source": [
        "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks running on a cluster can then add to it using the add method or the += operator. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.\n",
        "\n",
        "The code below shows an accumulator being used to add up the elements of an array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vpHngluAa00",
        "outputId": "071dfe17-5038-47a3-c3e9-5de5cfc6adf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum 15\n",
            "Sum 15\n",
            "Count 5\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
        "\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print('Sum',accum.value)\n",
        "\n",
        "accuSum=spark.sparkContext.accumulator(0)\n",
        "def countFun(x):\n",
        "    global accuSum\n",
        "    accuSum+=x\n",
        "rdd.foreach(countFun)\n",
        "print('Sum',accuSum.value)\n",
        "\n",
        "accumCount=spark.sparkContext.accumulator(0)\n",
        "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd2.foreach(lambda x:accumCount.add(1))\n",
        "print('Count',accumCount.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2mZzAxAa05"
      },
      "source": [
        "## Broadcast Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vge2fv0uAa06"
      },
      "source": [
        "Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
        "\n",
        "Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n",
        "\n",
        "Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWNN9YDAa08",
        "outputId": "fac453e0-c4ff-4c2f-fd98-d328b601d5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored data -> ['shreyash', 'rishabh', 'Shubham', 'Sagar', 'Mehul']\n",
            "Printing a particular element in RDD -> Shubham\n"
          ]
        }
      ],
      "source": [
        "words_new = spark.sparkContext.broadcast([\"shreyash\", \"rishabh\", \"Shubham\", \"Sagar\", \"Mehul\"])\n",
        "data = words_new.value\n",
        "print (f\"Stored data -> {data}\")  \n",
        "elem = words_new.value[2]\n",
        "print (f\"Printing a particular element in RDD -> {elem}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7lsKjR8Aa0-"
      },
      "source": [
        "### Tasks\n",
        "complete the following code to get output\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZUodJ7FAa0_",
        "outputId": "b2709b4d-c540-42ba-e749-2766b1c78796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+----------+\n",
            "|firstname|lastname|country|state     |\n",
            "+---------+--------+-------+----------+\n",
            "|James    |Smith   |USA    |California|\n",
            "|Michael  |Rose    |USA    |New York  |\n",
            "|Robert   |Williams|USA    |California|\n",
            "|Maria    |Jones   |USA    |Florida   |\n",
            "+---------+--------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark =  SparkSession.builder.appName(\"broadcast\").getOrCreate()\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates =spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXsiicXCAa1B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}